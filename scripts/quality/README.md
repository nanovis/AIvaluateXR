# Quality Benchmark
We used [perplexity](https://github.com/ggml-org/llama.cpp/tree/master/examples/perplexity) of Llama.cpp to test the quality of LLMs. We tested the **MMLU**, **Hellaswag**, **Winogrande**, **Arc-Challenge**, **wikitext** and Truthful-QA. Here are all the [datasets](https://github.com/nanovis/LoXR/tree/main/scripts/Android_devices/quality/datasets).

You can reproduce our experiment by following these steps:
## Usage
